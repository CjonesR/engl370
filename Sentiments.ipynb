{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6afd9f31-eca3-4aec-a2a5-27753d6ef55f",
   "metadata": {},
   "source": [
    "Load in all of the imports that are needed throughout the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9fcf1e12-7dfc-4cc2-a86b-ce5bdd56c88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/chasejones/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re, numpy as np, random\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download(\"vader_lexicon\")\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62df089d-da84-4d2d-9287-1af4f64c4c4f",
   "metadata": {},
   "source": [
    "Load all of the data for this corpora and then load the gabs. The bottom (print(len(gabs)) shows\n",
    "how many we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8df929de-8ada-4e25-8856-fd8fad5e7500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10316\n"
     ]
    }
   ],
   "source": [
    "# DATA\n",
    "# Load the gabs:\n",
    "with open('movies/Action/aliens.txt', 'r') as f:\n",
    "    gabs = f.readlines()\n",
    "\n",
    "# How many do we have?\n",
    "print(len(gabs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1975774-856c-4420-8eca-7796a3a42169",
   "metadata": {},
   "source": [
    "This code removes any links that are found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20880e38-147b-4a98-9eb9-ad628e2600ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = re.compile(r'<[^>]*>')\n",
    "html_free = [ re.sub(html, \" \", gab) for gab in gabs ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f92a3c-55cf-4bdc-a511-682b606c1c08",
   "metadata": {},
   "source": [
    "This cell is tokenizing the text.This is in a cell by itself because tokenizing takes time, \n",
    "and we only want to do it once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7f19612-3059-46c3-8f31-f047a3d30c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = []\n",
    "for i in html_free:\n",
    "    tokens = word_tokenize(i)\n",
    "    tokenized.append(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3300bc97-733f-4626-a86c-796326f8fb57",
   "metadata": {},
   "source": [
    "The cell below is an example of a list comprehension -- the code inside the square brackets, \n",
    "[] -- embedded inside two functions, first a length and then a print. It's simple, compact, and \n",
    "it allows me to change the number at the end and re-run the cell to \"map\" out the data a bit in my mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea2965ec-5893-4504-b8d3-aa53f1b155c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6819\n",
      "8901\n"
     ]
    }
   ],
   "source": [
    "print(len([i for i in tokenized if len(i) < 5]))\n",
    "print(len([i for i in tokenized if len(i) < 10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cc225d-a78a-42ab-8617-9312249546e3",
   "metadata": {},
   "source": [
    "I use numpys histogram here to get a more nuanced \"mapping\" of the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ac60b85-7e33-4336-a519-7e264cce0f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6409 1529 2046  330    1    0    0    0    0    1]\n",
      "[ 0.   3.9  7.8 11.7 15.6 19.5 23.4 27.3 31.2 35.1 39. ]\n"
     ]
    }
   ],
   "source": [
    "lengths = [ len(i) for i in tokenized ]\n",
    "counts, bins = np.histogram(lengths)\n",
    "print(counts)\n",
    "print(bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630ce417-0e82-4380-b1b2-e9fa24b511a4",
   "metadata": {},
   "source": [
    "This sets the posts to 5-10 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78b91baa-7622-4320-88e9-fbde87b0cfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "shorts = [post for post in tokenized if len(post) > 5 and len(post) < 11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbddefbb-b8f8-489e-8ec1-4ddf0eb275b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frantically . Like an indicator of her growing panic .\n",
      "chamber echoes with nightmarish sounds ... WHINE , CRASH ,\n",
      "A second torch cuts through . They move with machine\n",
      "surgical and anaethesiology equipment loom in the\n",
      "the Krupps munitions works and a truckstop casino in\n"
     ]
    }
   ],
   "source": [
    "for item in random.sample(shorts, 5):\n",
    "    print(\" \".join(item))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c40c71-e85a-4c42-9bde-bc9997f3e970",
   "metadata": {},
   "source": [
    "This sets the floor to 10, but unfortunately this will eliminate over half the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b007640-c639-4b0b-93db-ecd755b483c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is wedged into a tiny space between the driver 's seat\n"
     ]
    }
   ],
   "source": [
    "# List comprehension for gabs of greater than 10 words\n",
    "texts = [ post for post in tokenized if len(post) > 10 ]\n",
    "\n",
    "# Join the gabs back together because NLTK's sentiment expects it?\n",
    "joins = [ \" \".join(text) for text in texts ]\n",
    "\n",
    "# Let's see a random one of them:\n",
    "print(random.choice(joins))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa783a11-9f9f-491c-9a09-5d00acf902c1",
   "metadata": {},
   "source": [
    "This shows the poloarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c985295d-8fc5-47eb-b457-39525e07dc12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.543, 'pos': 0.457, 'compound': 0.6369}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How to get a score\n",
    "sia.polarity_scores(\"Python is the best programming language.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "261e3c25-3184-4a84-ac05-28a9e127e49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the eggs . She fires the flamethrower . The eggs are\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# What it looks like for our corpus:\n",
    "sample = random.choice(joins)\n",
    "print(sample)\n",
    "print(sia.polarity_scores(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "845899b3-15d8-4a0c-94af-54dcc29273ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INT . HOSPITAL ROOM - TIGHT ON RIPLEY - GATEWAY STATION 3\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# And now just getting the compound:\n",
    "sample = random.choice(joins)\n",
    "print(sample)\n",
    "print(sia.polarity_scores(sample)[\"compound\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7dc41c-92b1-41db-8b5f-820ba05e4777",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
